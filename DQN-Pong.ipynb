{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on Pong\n",
    "\n",
    "Done with reference to Denny Britz's DQN implementation found at\n",
    "https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A replay buffer for implementing experience replay\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: the size of the replay buffer, items will be evicted in a FIFO manner\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, is_terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state\n",
    "            action\n",
    "            reward\n",
    "            next_state\n",
    "            is_terminal\n",
    "            \n",
    "        Store experience into replay buffer, evicting old experience if buffer is full\n",
    "        \"\"\"\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append([state, action, reward, next_state, is_terminal])\n",
    "        \n",
    "    def sample(self, no_of_samples):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            no_of_sample: number of samples desired\n",
    "            \n",
    "        Return:\n",
    "            samples of length no_of_sample\n",
    "            \n",
    "        Samples from replay buffer\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(len(self.buffer), no_of_samples)\n",
    "        return np.array(self.buffer)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    W = tf.get_variable('W', initializer=tf.truncated_normal(shape, mean=0, stddev=0.1))\n",
    "    return W\n",
    "\n",
    "def bias_init(shape):\n",
    "    b = tf.get_variable('b', initializer=tf.constant(0.1, shape=shape))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork:\n",
    "    def __init__(self, frame_size, no_of_frame, no_of_actions, global_step, scope='Estimator', summaries_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_size: width and height of a single frame\n",
    "            no_of_frame: number of frames stacked\n",
    "            no_of_actions: number of actions (i.e output neurons), this varies from game to game\n",
    "            scope: name of scope. Used to distinguish target and estimator network\n",
    "        \"\"\"\n",
    "        self.scope = scope\n",
    "        self.summary_writer = None\n",
    "        self.step = 0\n",
    "        with tf.variable_scope(scope):\n",
    "            # build summary writer\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "            \n",
    "            \n",
    "            self.X = tf.placeholder(tf.uint8, shape=[None, frame_size, frame_size, no_of_frame], name='X')\n",
    "            self.targets = tf.placeholder(tf.float32, shape=[None], name='targets')\n",
    "            self.selected_actions = tf.placeholder(tf.int32, shape=[None], name='actions')\n",
    "            X = tf.to_float(self.X) / 255.0\n",
    "            batch_size = tf.shape(self.X)[0]\n",
    "            \n",
    "            with tf.variable_scope('conv1'):\n",
    "                W1 = weights_init([8,8, no_of_frame, 32])\n",
    "                b1 = bias_init([32])\n",
    "                conv1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1,4,4,1], padding='VALID') + b1)\n",
    "            with tf.variable_scope('conv2'):\n",
    "                W2 = weights_init([4,4, 32, 64])\n",
    "                b2 = bias_init([64])\n",
    "                conv2 = tf.nn.relu(tf.nn.conv2d(conv1, W2, strides=[1,2,2,1], padding='VALID') + b2)\n",
    "            with tf.variable_scope('conv3'):\n",
    "                W3 = weights_init([3,3, 64, 64])\n",
    "                b3 = bias_init([64])\n",
    "                conv3 = tf.nn.relu(tf.nn.conv2d(conv2, W3, strides=[1,1,1,1], padding='VALID') + b3)\n",
    "                final_conv_width = (((frame_size - 8)//4 + 1 - 4)//2 + 1 - 3) + 1\n",
    "            with tf.variable_scope('fc4'):\n",
    "                W4 = weights_init([final_conv_width**2 * 64, 512])\n",
    "                b4 = bias_init([512])\n",
    "                flattened = tf.reshape(conv3, [-1, final_conv_width**2 * 64])\n",
    "                fc4 = tf.nn.relu(tf.matmul(flattened, W4) + b4)\n",
    "            with tf.variable_scope('fc5'):\n",
    "                W5 = weights_init([512, no_of_actions])\n",
    "                b5 = bias_init([no_of_actions])\n",
    "                self.predictions = tf.matmul(fc4, W5) + b5\n",
    "\n",
    "            # compute loss\n",
    "            # we need to extract the action values of selected actions\n",
    "            # to do that, we will flatten the predictions into a 1d array\n",
    "            # we then transform the action index to an index compatible with this 1d array\n",
    "            # we transform the action index by adding the action index to the offset for each row\n",
    "            # reference from\n",
    "            # https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "            with tf.variable_scope('loss'):\n",
    "                gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.selected_actions\n",
    "                self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "                self.losses = tf.squared_difference(self.targets, self.action_predictions)\n",
    "                self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "            # create train op\n",
    "            # I am neglecting error clipping that was used in the paper\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "                self.train_op = self.optimizer.minimize(self.loss, \n",
    "                                                        var_list=[W1, b1, W2, b2, W3, b3, W4, b4, W5, b5],\n",
    "                                                        global_step=global_step)\n",
    "            \n",
    "            # Summaries for Tensorboard\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar(\"loss\", self.loss),\n",
    "                tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "                tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "                tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "            ])\n",
    "            \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, frame_size, frame_size, no_of_frames]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, no_of_actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X: s })\n",
    "\n",
    "    def update(self, sess, s, a, targets):\n",
    "        \"\"\"\n",
    "        Updates the network towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, frame_size, frame_size, no_of_frames]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          targets: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X: s, self.targets: targets, self.selected_actions: a }\n",
    "        summaries, _, loss, step = sess.run(\n",
    "            [self.summaries, self.train_op, self.loss, global_step],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, step)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "def copy_model_parameters(sess, network1, network2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    n1_params = [t for t in tf.trainable_variables() if t.name.startswith(network1.scope)]\n",
    "    n1_params = sorted(n1_params, key=lambda v: v.name)\n",
    "    n2_params = [t for t in tf.trainable_variables() if t.name.startswith(network2.scope)]\n",
    "    n2_params = sorted(n2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for n1_v, n2_v in zip(n1_params, n2_params):\n",
    "        op = n2_v.assign(n1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(sess, env, q_network, state, epsilon, e_greedy=True):\n",
    "    if e_greedy and np.random.uniform() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action_values = q_network.predict(sess, np.expand_dims(state, 0))\n",
    "        return np.argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(sess,\n",
    "              env,\n",
    "              q_network,\n",
    "              target_network,\n",
    "              state_processor,\n",
    "              num_episodes,\n",
    "              global_step,\n",
    "              experiment_dir,\n",
    "              replay_buffer,\n",
    "              buffer_init_size,\n",
    "              target_interval,\n",
    "              frame_skip,\n",
    "              epsilon_start,\n",
    "              epsilon_end,\n",
    "              epsilon_decay_length,\n",
    "              gamma,\n",
    "              batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        buffer_init_size: no of frames used to initialize replay buffer\n",
    "        frame_skip: no. of frames to skip between decision (paper used 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    episode_lengths=np.zeros(num_episodes)\n",
    "    episode_rewards=np.zeros(num_episodes)\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "    total_t = sess.run(global_step)\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_length)\n",
    "    \n",
    "    last_4_frame = []\n",
    "    \n",
    "    # init replay buffer\n",
    "    print(\"Populating replay memory...\")\n",
    "    observation = env.reset()\n",
    "    observation = state_processor.process(sess, observation)\n",
    "    # populate last 4 frames\n",
    "    last_4_frame = [observation.tolist()] * 4\n",
    "    \n",
    "    for i in range(buffer_init_size):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"\\rpopulating buffer: %d\" %i, end=\"\")\n",
    "        current_state = np.stack(last_4_frame, axis=2)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        observation = state_processor.process(sess, observation)\n",
    "        last_4_frame.pop(0)\n",
    "        last_4_frame.append(observation)\n",
    "        next_state = np.stack(last_4_frame, axis=2)\n",
    "        replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "            observation = state_processor.process(sess, observation)\n",
    "            # populate last 4 frames\n",
    "            last_4_frame = [observation.tolist()] * 4\n",
    "            \n",
    "        \n",
    "    print(\"\\nPopulated replay memory\")\n",
    "    \n",
    "    # this can only be done after populating replay buffer\n",
    "    should_record = False\n",
    "    # set up env to record video near the end of training\n",
    "    env = Monitor(env, directory='./', resume=True, video_callable=lambda count: should_record and count % 10 == 0)\n",
    "    \n",
    "    \n",
    "    action = None\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        # if is last 4 episode, record video\n",
    "        if i_episode >= num_episodes - 4 or i_episode % 5000 == 0:\n",
    "            should_record = False\n",
    "        else:\n",
    "            should_record = False\n",
    "\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        observation = state_processor.process(sess, observation)\n",
    "        last_4_frame = [observation.tolist()] * 4\n",
    "        loss = None\n",
    "        current_state = np.stack(last_4_frame, axis=2)\n",
    "        for t in itertools.count():\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_length-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_network.summary_writer.add_summary(episode_summary, total_t)\n",
    "            \n",
    "            # update the target network\n",
    "            if total_t % target_interval == 0:\n",
    "                copy_model_parameters(sess, q_network, target_network)\n",
    "                \n",
    "            \n",
    "            if t % frame_skip == 0:\n",
    "                # only make decision every k (frame_skip) steps\n",
    "                action = select_action(sess, env, q_network, current_state, epsilon, e_greedy=True)\n",
    "                \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            observation = state_processor.process(sess, observation)\n",
    "            last_4_frame.pop(0)\n",
    "            last_4_frame.append(observation)\n",
    "            next_state = np.stack(last_4_frame, axis=2)\n",
    "            replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update statistics\n",
    "            episode_rewards[i_episode] += reward\n",
    "            episode_lengths[i_episode] = t\n",
    "            \n",
    "            # train\n",
    "            samples = replay_buffer.sample(batch_size)\n",
    "            states = []\n",
    "            actions = []\n",
    "            targets = []\n",
    "            for s, a, r, next_s, d in samples:\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                # compute target\n",
    "                qs = target_network.predict(sess, np.expand_dims(next_s, 0))\n",
    "                qmax = np.max(qs)\n",
    "                if r > 0:\n",
    "                    r_clip = 1\n",
    "                elif r < 0:\n",
    "                    r_clip = -1\n",
    "                else:\n",
    "                    r_clip = 0\n",
    "                targets.append(r_clip + gamma * qmax * (1-int(d)))\n",
    "            loss = q_network.update(sess, states, actions, targets)\n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            current_state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        print(\"\\nEpisode {}, Reward: {}\".format(i_episode + 1, episode_rewards[i_episode]))\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_network.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_network.summary_writer.flush()\n",
    "    \n",
    "    env.render(close=True)\n",
    "    env.close()\n",
    "    return episode_rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-12 17:48:48,602] Making new env: Pong-v0\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /home/aiskyscraper/dqn/experiments/Pong-v0/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/aiskyscraper/dqn/experiments/Pong-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-12 17:48:49,745] Restoring parameters from /home/aiskyscraper/dqn/experiments/Pong-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "populating buffer: 2000"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "q_network = Qnetwork(84, 4, env.action_space.n, global_step, scope='estimator', summaries_dir=experiment_dir)\n",
    "target_network = Qnetwork(84, 4, env.action_space.n, global_step, scope='target')\n",
    "\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=500000)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # copy network to target\n",
    "    copy_model_parameters(sess, q_network, target_network)\n",
    "    rewards, lengths = train_dqn(sess=sess,\n",
    "              env=env,\n",
    "              q_network=q_network,\n",
    "              target_network=target_network,\n",
    "              state_processor=state_processor,\n",
    "              num_episodes=5000,\n",
    "              global_step=global_step,\n",
    "              experiment_dir=experiment_dir,\n",
    "              replay_buffer=replay_buffer,\n",
    "              buffer_init_size=100000,\n",
    "              target_interval=10000,\n",
    "              frame_skip=1,\n",
    "              epsilon_start=1,\n",
    "              epsilon_end=0.1,\n",
    "              epsilon_decay_length=500000,\n",
    "              gamma=0.99,\n",
    "              batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
