{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on pacman\n",
    "\n",
    "Done with reference to Denny Britz's DQN implementation found at\n",
    "https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaixiangteo/rl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A replay buffer for implementing experience replay\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: the size of the replay buffer, items will be evicted in a FIFO manner\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, is_terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state\n",
    "            action\n",
    "            reward\n",
    "            next_state\n",
    "            is_terminal\n",
    "            \n",
    "        Store experience into replay buffer, evicting old experience if buffer is full\n",
    "        \"\"\"\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append([state, action, reward, next_state, is_terminal])\n",
    "        \n",
    "    def sample(self, no_of_samples):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            no_of_sample: number of samples desired\n",
    "            \n",
    "        Return:\n",
    "            samples of length no_of_sample\n",
    "            \n",
    "        Samples from replay buffer\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(len(self.buffer), no_of_samples)\n",
    "        return np.array(self.buffer)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 5, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    W = tf.get_variable('W', initializer=tf.truncated_normal(shape, mean=0, stddev=0.1))\n",
    "    return W\n",
    "\n",
    "def bias_init(shape):\n",
    "    b = tf.get_variable('b', initializer=tf.constant(0.1, shape=shape))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork:\n",
    "    def __init__(self, frame_size, no_of_frame, no_of_actions, global_step, scope='Estimator', summaries_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_size: width and height of a single frame\n",
    "            no_of_frame: number of frames stacked\n",
    "            no_of_actions: number of actions (i.e output neurons), this varies from game to game\n",
    "            scope: name of scope. Used to distinguish target and estimator network\n",
    "        \"\"\"\n",
    "        self.scope = scope\n",
    "        self.summary_writer = None\n",
    "        self.step = 0\n",
    "        with tf.variable_scope(scope):\n",
    "            # build summary writer\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "            \n",
    "            \n",
    "            self.X = tf.placeholder(tf.uint8, shape=[None, frame_size, frame_size, no_of_frame], name='X')\n",
    "            self.targets = tf.placeholder(tf.float32, shape=[None], name='targets')\n",
    "            self.selected_actions = tf.placeholder(tf.int32, shape=[None], name='actions')\n",
    "            X = tf.to_float(self.X) / 255.0\n",
    "            batch_size = tf.shape(self.X)[0]\n",
    "            \n",
    "            with tf.variable_scope('conv1'):\n",
    "                W1 = weights_init([8,8, no_of_frame, 32])\n",
    "                b1 = bias_init([32])\n",
    "                conv1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1,4,4,1], padding='VALID') + b1)\n",
    "            with tf.variable_scope('conv2'):\n",
    "                W2 = weights_init([4,4, 32, 64])\n",
    "                b2 = bias_init([64])\n",
    "                conv2 = tf.nn.relu(tf.nn.conv2d(conv1, W2, strides=[1,2,2,1], padding='VALID') + b2)\n",
    "            with tf.variable_scope('conv3'):\n",
    "                W3 = weights_init([3,3, 64, 64])\n",
    "                b3 = bias_init([64])\n",
    "                conv3 = tf.nn.relu(tf.nn.conv2d(conv2, W3, strides=[1,1,1,1], padding='VALID') + b3)\n",
    "                final_conv_width = (((frame_size - 8)//4 + 1 - 4)//2 + 1 - 3) + 1\n",
    "            with tf.variable_scope('fc4'):\n",
    "                W4 = weights_init([final_conv_width**2 * 64, 512])\n",
    "                b4 = bias_init([512])\n",
    "                flattened = tf.reshape(conv3, [-1, final_conv_width**2 * 64])\n",
    "                fc4 = tf.nn.relu(tf.matmul(flattened, W4) + b4)\n",
    "            with tf.variable_scope('fc5'):\n",
    "                W5 = weights_init([512, no_of_actions])\n",
    "                b5 = bias_init([no_of_actions])\n",
    "                self.predictions = tf.matmul(fc4, W5) + b5\n",
    "\n",
    "            # compute loss\n",
    "            # we need to extract the action values of selected actions\n",
    "            # to do that, we will flatten the predictions into a 1d array\n",
    "            # we then transform the action index to an index compatible with this 1d array\n",
    "            # we transform the action index by adding the action index to the offset for each row\n",
    "            # reference from\n",
    "            # https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "            with tf.variable_scope('loss'):\n",
    "                gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.selected_actions\n",
    "                self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "                self.losses = tf.squared_difference(self.targets, self.action_predictions)\n",
    "                self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "            # create train op\n",
    "            # I am neglecting error clipping that was used in the paper\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "                self.train_op = self.optimizer.minimize(self.loss, \n",
    "                                                        var_list=[W1, b1, W2, b2, W3, b3, W4, b4, W5, b5],\n",
    "                                                        global_step=global_step)\n",
    "            \n",
    "            # Summaries for Tensorboard\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar(\"loss\", self.loss),\n",
    "                tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "                tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "                tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "            ])\n",
    "            \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, frame_size, frame_size, no_of_frames]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, no_of_actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X: s })\n",
    "\n",
    "    def update(self, sess, s, a, targets):\n",
    "        \"\"\"\n",
    "        Updates the network towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, frame_size, frame_size, no_of_frames]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          targets: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X: s, self.targets: targets, self.selected_actions: a }\n",
    "        summaries, _, loss, step = sess.run(\n",
    "            [self.summaries, self.train_op, self.loss, global_step],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, step)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that Qnetwork works correctly\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\")\n",
    "q = Qnetwork(84, 4, 4, global_step, scope='test')\n",
    "sp = StateProcessor()\n",
    "env = gym.make('MsPacman-v0')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] *4, axis=2)\n",
    "    print(observation.shape)\n",
    "    observations = np.array([observation] * 32)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(q.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.ones(32)\n",
    "    a = np.array(np.random.randint(4, size=(32)))\n",
    "    print(q.update(sess, observations, a, y))\n",
    "    print(sess.run(global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "def copy_model_parameters(sess, network1, network2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    n1_params = [t for t in tf.trainable_variables() if t.name.startswith(network1.scope)]\n",
    "    n1_params = sorted(n1_params, key=lambda v: v.name)\n",
    "    n2_params = [t for t in tf.trainable_variables() if t.name.startswith(network2.scope)]\n",
    "    n2_params = sorted(n2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for n1_v, n2_v in zip(n1_params, n2_params):\n",
    "        op = n2_v.assign(n1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaixiangteo/rl/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    q1 = Qnetwork(84, 4, 4, scope='test1')\n",
    "    q2 = Qnetwork(84, 4, 4, scope='test2')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    copy_model_parameters(sess, q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,4], [3,4,6]]\n",
    "np.stack([a]*2, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2], [3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([a] * 4, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(sess, env, q_network, state, epsilon, e_greedy=True):\n",
    "    if e_greedy and np.random.uniform() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action_values = q_network.predict(sess, np.expand_dims(state, 0))\n",
    "        return np.argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(sess,\n",
    "              env,\n",
    "              q_network,\n",
    "              target_network,\n",
    "              state_processor,\n",
    "              num_episodes,\n",
    "              global_step,\n",
    "              experiment_dir,\n",
    "              replay_buffer,\n",
    "              buffer_init_size,\n",
    "              target_interval,\n",
    "              frame_skip,\n",
    "              epsilon_start,\n",
    "              epsilon_end,\n",
    "              epsilon_decay_length,\n",
    "              gamma,\n",
    "              batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        buffer_init_size: no of frames used to initialize replay buffer\n",
    "        frame_skip: no. of frames to skip between decision (paper used 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    episode_lengths=np.zeros(num_episodes)\n",
    "    episode_rewards=np.zeros(num_episodes)\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "    total_t = sess.run(global_step)\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_length)\n",
    "    \n",
    "    last_4_frame = []\n",
    "    \n",
    "    # init replay buffer\n",
    "    print(\"Populating replay memory...\")\n",
    "    observation = env.reset()\n",
    "    observation = state_processor.process(sess, observation)\n",
    "    # populate last 4 frames\n",
    "    last_4_frame = [observation.tolist()] * 4\n",
    "    \n",
    "    for i in range(buffer_init_size):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"\\rpopulating buffer: %d\" %i, end=\"\")\n",
    "        current_state = np.stack(last_4_frame, axis=2)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = state_processor.process(sess, observation)\n",
    "        last_4_frame.pop(0)\n",
    "        last_4_frame.append(observation)\n",
    "        next_state = np.stack(last_4_frame, axis=2)\n",
    "        replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "            observation = state_processor.process(sess, observation)\n",
    "            # populate last 4 frames\n",
    "            last_4_frame = [observation.tolist()] * 4\n",
    "            \n",
    "        \n",
    "    print(\"\\nPopulated replay memory\")\n",
    "    \n",
    "    # this can only be done after populating replay buffer\n",
    "    should_record = False\n",
    "    # set up env to record video near the end of training\n",
    "    env = Monitor(env, directory='./', resume=True, video_callable=lambda count: should_record and count % 10 == 0)\n",
    "    \n",
    "    \n",
    "    action = None\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        # if is last 4 episode, record video\n",
    "        if i_episode >= num_episodes - 4 or i_episode % 200 == 0:\n",
    "            should_record = True\n",
    "        else:\n",
    "            should_record = False\n",
    "\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        observation = state_processor.process(sess, observation)\n",
    "        last_4_frame = [observation.tolist()] * 4\n",
    "        loss = None\n",
    "        current_state = np.stack(last_4_frame, axis=2)\n",
    "        for t in itertools.count():\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_length-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_network.summary_writer.add_summary(episode_summary, total_t)\n",
    "            \n",
    "            # update the target network\n",
    "            if total_t % target_interval == 0:\n",
    "                copy_model_parameters(sess, q_network, target_network)\n",
    "                \n",
    "            \n",
    "            if t % frame_skip == 0:\n",
    "                # only make decision every k (frame_skip) steps\n",
    "                action = select_action(sess, env, q_network, current_state, epsilon, e_greedy=True)\n",
    "                \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            observation = state_processor.process(sess, observation)\n",
    "            last_4_frame.pop(0)\n",
    "            last_4_frame.append(observation)\n",
    "            next_state = np.stack(last_4_frame, axis=2)\n",
    "            replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update statistics\n",
    "            episode_rewards[i_episode] += reward\n",
    "            episode_lengths[i_episode] = t\n",
    "            \n",
    "            # train\n",
    "            samples = replay_buffer.sample(batch_size)\n",
    "            states = []\n",
    "            actions = []\n",
    "            targets = []\n",
    "            for s, a, r, next_s, d in samples:\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                # compute target\n",
    "                qs = target_network.predict(sess, np.expand_dims(next_s, 0))\n",
    "                qmax = np.max(qs)\n",
    "                if r > 0:\n",
    "                    r_clip = 1\n",
    "                elif r < 0:\n",
    "                    r_clip = -1\n",
    "                else:\n",
    "                    r_clip = 0\n",
    "                targets.append(r_clip + gamma * qmax * (1-int(d)))\n",
    "            loss = q_network.update(sess, states, actions, targets)\n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            current_state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        print(\"\\nEpisode {}, Reward: {}\".format(i_episode + 1, episode_rewards[i_episode]))\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_network.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_network.summary_writer.flush()\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-03 12:01:11,751] Making new env: MsPacman-v0\n",
      "/Users/kaixiangteo/rl/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /Users/kaixiangteo/rl/dqn/experiments/MsPacman-v0/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/kaixiangteo/rl/dqn/experiments/MsPacman-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-03 12:01:16,075] Restoring parameters from /Users/kaixiangteo/rl/dqn/experiments/MsPacman-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "populating buffer: 9000\n",
      "Populated replay memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-03 12:02:15,724] Starting new video recorder writing to /Users/kaixiangteo/rl/dqn/openaigym.video.0.71332.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 648 (390457) @ Episode 1/1000, loss: 0.33811601996421814\n",
      "Episode 1, Reward: 350.0\n",
      "Step 607 (391065) @ Episode 2/1000, loss: 0.33033081889152527\n",
      "Episode 2, Reward: 340.0\n",
      "Step 841 (391907) @ Episode 3/1000, loss: 0.12275858223438263\n",
      "Episode 3, Reward: 910.0\n",
      "Step 546 (392454) @ Episode 4/1000, loss: 0.11942858248949051\n",
      "Episode 4, Reward: 270.0\n",
      "Step 1151 (393606) @ Episode 5/1000, loss: 0.11109539866447449\n",
      "Episode 5, Reward: 840.0\n",
      "Step 482 (394089) @ Episode 6/1000, loss: 0.21123261749744415\n",
      "Episode 6, Reward: 220.0\n",
      "Step 581 (394671) @ Episode 7/1000, loss: 0.248263359069824225\n",
      "Episode 7, Reward: 330.0\n",
      "Step 980 (395652) @ Episode 8/1000, loss: 0.52344572544097986\n",
      "Episode 8, Reward: 710.0\n",
      "Step 700 (396353) @ Episode 9/1000, loss: 0.083173424005508425\n",
      "Episode 9, Reward: 380.0\n",
      "Step 857 (397211) @ Episode 10/1000, loss: 0.09126394987106323\n",
      "Episode 10, Reward: 690.0\n",
      "Step 646 (397858) @ Episode 11/1000, loss: 0.232926636934280464\n",
      "Episode 11, Reward: 440.0\n",
      "Step 675 (398534) @ Episode 12/1000, loss: 0.15642563998699188\n",
      "Episode 12, Reward: 450.0\n",
      "Step 968 (399503) @ Episode 13/1000, loss: 0.19340363144874573\n",
      "Episode 13, Reward: 940.0\n",
      "Step 1042 (400546) @ Episode 14/1000, loss: 0.33430588245391846\n",
      "Episode 14, Reward: 1080.0\n",
      "Step 644 (401191) @ Episode 15/1000, loss: 0.23318852484226227\n",
      "Episode 15, Reward: 320.0\n",
      "Step 569 (401761) @ Episode 16/1000, loss: 0.19076773524284363\n",
      "Episode 16, Reward: 430.0\n",
      "Step 858 (402620) @ Episode 17/1000, loss: 0.63813614845275887\n",
      "Episode 17, Reward: 640.0\n",
      "Step 1033 (403654) @ Episode 18/1000, loss: 0.30446064472198486\n",
      "Episode 18, Reward: 770.0\n",
      "Step 931 (404586) @ Episode 19/1000, loss: 1.20294153690338137\n",
      "Episode 19, Reward: 1240.0\n",
      "Step 650 (405237) @ Episode 20/1000, loss: 0.301414400339126635\n",
      "Episode 20, Reward: 340.0\n",
      "Step 1057 (406295) @ Episode 21/1000, loss: 0.20432776212692268\n",
      "Episode 21, Reward: 870.0\n",
      "Step 830 (407126) @ Episode 22/1000, loss: 0.08405064046382904\n",
      "Episode 22, Reward: 450.0\n",
      "Step 681 (407808) @ Episode 23/1000, loss: 0.482378840446472174\n",
      "Episode 23, Reward: 460.0\n",
      "Step 547 (408356) @ Episode 24/1000, loss: 0.235601961612701426\n",
      "Episode 24, Reward: 380.0\n",
      "Step 876 (409233) @ Episode 25/1000, loss: 0.20159286260604858\n",
      "Episode 25, Reward: 780.0\n",
      "Step 470 (409704) @ Episode 26/1000, loss: 0.20668256282806396\n",
      "Episode 26, Reward: 300.0\n",
      "Step 779 (410484) @ Episode 27/1000, loss: 0.38080751895904544\n",
      "Episode 27, Reward: 560.0\n",
      "Step 608 (411093) @ Episode 28/1000, loss: 0.94461369514465336\n",
      "Episode 28, Reward: 370.0\n",
      "Step 656 (411750) @ Episode 29/1000, loss: 0.20014791190624237\n",
      "Episode 29, Reward: 380.0\n",
      "Step 697 (412448) @ Episode 30/1000, loss: 0.17523235082626343\n",
      "Episode 30, Reward: 330.0\n",
      "Step 450 (412899) @ Episode 31/1000, loss: 0.58727169036865236\n",
      "Episode 31, Reward: 260.0\n",
      "Step 849 (413749) @ Episode 32/1000, loss: 0.72159147262573246\n",
      "Episode 32, Reward: 490.0\n",
      "Step 583 (414333) @ Episode 33/1000, loss: 0.255367159843444885\n",
      "Episode 33, Reward: 260.0\n",
      "Step 655 (414989) @ Episode 34/1000, loss: 0.55449748039245668\n",
      "Episode 34, Reward: 350.0\n",
      "Step 460 (415450) @ Episode 35/1000, loss: 0.13634915649890918\n",
      "Episode 35, Reward: 300.0\n",
      "Step 627 (416078) @ Episode 36/1000, loss: 1.32005453109741222\n",
      "Episode 36, Reward: 360.0\n",
      "Step 605 (416684) @ Episode 37/1000, loss: 0.34082826972007757\n",
      "Episode 37, Reward: 470.0\n",
      "Step 1363 (418048) @ Episode 38/1000, loss: 0.19351395964622498\n",
      "Episode 38, Reward: 580.0\n",
      "Step 557 (418606) @ Episode 39/1000, loss: 0.21393752098083496\n",
      "Episode 39, Reward: 410.0\n",
      "Step 709 (419316) @ Episode 40/1000, loss: 0.20672234892845154\n",
      "Episode 40, Reward: 380.0\n",
      "Step 596 (419913) @ Episode 41/1000, loss: 0.117454275488853454\n",
      "Episode 41, Reward: 400.0\n",
      "Step 535 (420449) @ Episode 42/1000, loss: 0.46895146369934084\n",
      "Episode 42, Reward: 390.0\n",
      "Step 700 (421150) @ Episode 43/1000, loss: 0.27709791064262397\n",
      "Episode 43, Reward: 430.0\n",
      "Step 544 (421695) @ Episode 44/1000, loss: 0.20727333426475525\n",
      "Episode 44, Reward: 350.0\n",
      "Step 930 (422626) @ Episode 45/1000, loss: 0.23601207137107855\n",
      "Episode 45, Reward: 910.0\n",
      "Step 641 (423268) @ Episode 46/1000, loss: 0.31176802515983585\n",
      "Episode 46, Reward: 550.0\n",
      "Step 797 (424066) @ Episode 47/1000, loss: 0.37604391574859627\n",
      "Episode 47, Reward: 610.0\n",
      "Step 467 (424534) @ Episode 48/1000, loss: 0.25733056664466865\n",
      "Episode 48, Reward: 290.0\n",
      "Step 468 (425003) @ Episode 49/1000, loss: 0.16590762138366706\n",
      "Episode 49, Reward: 250.0\n",
      "Step 598 (425602) @ Episode 50/1000, loss: 0.24088846147060394\n",
      "Episode 50, Reward: 390.0\n",
      "Step 621 (426224) @ Episode 51/1000, loss: 0.24168497323989868\n",
      "Episode 51, Reward: 290.0\n",
      "Step 911 (427136) @ Episode 52/1000, loss: 0.21168982982635498\n",
      "Episode 52, Reward: 490.0\n",
      "Step 683 (427820) @ Episode 53/1000, loss: 0.18828384578227997\n",
      "Episode 53, Reward: 1070.0\n",
      "Step 591 (428412) @ Episode 54/1000, loss: 0.30465930700302124\n",
      "Episode 54, Reward: 300.0\n",
      "Step 1080 (429493) @ Episode 55/1000, loss: 0.31621241569519043\n",
      "Episode 55, Reward: 630.0\n",
      "Step 636 (430130) @ Episode 56/1000, loss: 1.68071031570434578\n",
      "Episode 56, Reward: 440.0\n",
      "Step 750 (430881) @ Episode 57/1000, loss: 0.26022326946258545\n",
      "Episode 57, Reward: 860.0\n",
      "Step 482 (431364) @ Episode 58/1000, loss: 0.23215018212795258\n",
      "Episode 58, Reward: 270.0\n",
      "Step 540 (431905) @ Episode 59/1000, loss: 0.47495740652084354\n",
      "Episode 59, Reward: 280.0\n",
      "Step 616 (432522) @ Episode 60/1000, loss: 0.21564143896102905\n",
      "Episode 60, Reward: 310.0\n",
      "Step 686 (433209) @ Episode 61/1000, loss: 0.22237923741340637\n",
      "Episode 61, Reward: 450.0\n",
      "Step 663 (433873) @ Episode 62/1000, loss: 14.4348497390747074\n",
      "Episode 62, Reward: 390.0\n",
      "Step 536 (434410) @ Episode 63/1000, loss: 0.40883332490921027\n",
      "Episode 63, Reward: 330.0\n",
      "Step 558 (434969) @ Episode 64/1000, loss: 0.24565641582012177\n",
      "Episode 64, Reward: 330.0\n",
      "Step 896 (435866) @ Episode 65/1000, loss: 0.33617582917213444\n",
      "Episode 65, Reward: 490.0\n",
      "Step 460 (436327) @ Episode 66/1000, loss: 0.48741146922111515\n",
      "Episode 66, Reward: 330.0\n",
      "Step 578 (436906) @ Episode 67/1000, loss: 0.19343236088752747\n",
      "Episode 67, Reward: 350.0\n",
      "Step 494 (437401) @ Episode 68/1000, loss: 0.23434980213642125\n",
      "Episode 68, Reward: 280.0\n",
      "Step 527 (437929) @ Episode 69/1000, loss: 0.14135734736919403\n",
      "Episode 69, Reward: 210.0\n",
      "Step 544 (438474) @ Episode 70/1000, loss: 0.404639840126037666\n",
      "Episode 70, Reward: 280.0\n",
      "Step 760 (439235) @ Episode 71/1000, loss: 2.47479867935180663\n",
      "Episode 71, Reward: 500.0\n",
      "Step 670 (439906) @ Episode 72/1000, loss: 0.284523010253906254\n",
      "Episode 72, Reward: 340.0\n",
      "Step 335 (440242) @ Episode 73/1000, loss: 0.35845279693603516\n",
      "Episode 73, Reward: 110.0\n",
      "Step 486 (440729) @ Episode 74/1000, loss: 0.27763056755065927\n",
      "Episode 74, Reward: 290.0\n",
      "Step 622 (441352) @ Episode 75/1000, loss: 0.19338537752628326\n",
      "Episode 75, Reward: 440.0\n",
      "Step 782 (442135) @ Episode 76/1000, loss: 0.646397352218627966\n",
      "Episode 76, Reward: 460.0\n",
      "Step 821 (442957) @ Episode 77/1000, loss: 0.23821136355400085\n",
      "Episode 77, Reward: 640.0\n",
      "Step 787 (443745) @ Episode 78/1000, loss: 0.22195972502231598\n",
      "Episode 78, Reward: 450.0\n",
      "Step 609 (444355) @ Episode 79/1000, loss: 0.20756816864013672\n",
      "Episode 79, Reward: 270.0\n",
      "Step 545 (444901) @ Episode 80/1000, loss: 0.21443930268287662\n",
      "Episode 80, Reward: 320.0\n",
      "Step 700 (445602) @ Episode 81/1000, loss: 0.35016116499900823\n",
      "Episode 81, Reward: 470.0\n",
      "Step 740 (446343) @ Episode 82/1000, loss: 0.90482509136199954\n",
      "Episode 82, Reward: 560.0\n",
      "Step 527 (446871) @ Episode 83/1000, loss: 0.25800874829292397\n",
      "Episode 83, Reward: 310.0\n",
      "Step 570 (447442) @ Episode 84/1000, loss: 0.25881052017211914\n",
      "Episode 84, Reward: 370.0\n",
      "Step 1123 (448566) @ Episode 85/1000, loss: 0.43501430749893195\n",
      "Episode 85, Reward: 760.0\n",
      "Step 541 (449108) @ Episode 86/1000, loss: 2.78949809074401864\n",
      "Episode 86, Reward: 440.0\n",
      "Step 398 (449507) @ Episode 87/1000, loss: 0.23795634508132935\n",
      "Episode 87, Reward: 210.0\n",
      "Step 669 (450177) @ Episode 88/1000, loss: 0.286941945552825956\n",
      "Episode 88, Reward: 540.0\n",
      "Step 628 (450806) @ Episode 89/1000, loss: 0.23884546756744385\n",
      "Episode 89, Reward: 460.0\n",
      "Step 459 (451266) @ Episode 90/1000, loss: 0.14612109959125523\n",
      "Episode 90, Reward: 270.0\n",
      "Step 415 (451682) @ Episode 91/1000, loss: 0.27959740161895757\n",
      "Episode 91, Reward: 210.0\n",
      "Step 807 (452490) @ Episode 92/1000, loss: 0.13850882649421692\n",
      "Episode 92, Reward: 590.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 490 (452981) @ Episode 93/1000, loss: 0.18422834575176244\n",
      "Episode 93, Reward: 290.0\n",
      "Step 537 (453519) @ Episode 94/1000, loss: 0.18147447705268864\n",
      "Episode 94, Reward: 290.0\n",
      "Step 615 (454135) @ Episode 95/1000, loss: 0.64716476202011117\n",
      "Episode 95, Reward: 780.0\n",
      "Step 641 (454777) @ Episode 96/1000, loss: 0.23476032912731173\n",
      "Episode 96, Reward: 400.0\n",
      "Step 590 (455368) @ Episode 97/1000, loss: 0.13776515424251556\n",
      "Episode 97, Reward: 310.0\n",
      "Step 1046 (456415) @ Episode 98/1000, loss: 0.28912645578384456\n",
      "Episode 98, Reward: 1370.0\n",
      "Step 481 (456897) @ Episode 99/1000, loss: 0.46993118524551395\n",
      "Episode 99, Reward: 260.0\n",
      "Step 763 (457661) @ Episode 100/1000, loss: 0.21435143053531647\n",
      "Episode 100, Reward: 400.0\n",
      "Step 716 (458378) @ Episode 101/1000, loss: 10.9574413299560554\n",
      "Episode 101, Reward: 410.0\n",
      "Step 532 (458911) @ Episode 102/1000, loss: 0.22247044742107397\n",
      "Episode 102, Reward: 270.0\n",
      "Step 1007 (459919) @ Episode 103/1000, loss: 0.21947227418422703\n",
      "Episode 103, Reward: 950.0\n",
      "Step 297 (460217) @ Episode 104/1000, loss: 0.41654568910598755"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-93f0d0205d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mepsilon_decay_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               batch_size=32)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-37f62c3cf2df>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(sess, env, q_network, target_network, state_processor, num_episodes, global_step, experiment_dir, replay_buffer, buffer_init_size, target_interval, frame_skip, epsilon_start, epsilon_end, epsilon_decay_length, gamma, batch_size)\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mr_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_clip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mqmax\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e35983556c95>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, targets)\u001b[0m\n\u001b[1;32m    108\u001b[0m         summaries, _, loss, step = sess.run(\n\u001b[1;32m    109\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "env = gym.make('MsPacman-v0')\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "q_network = Qnetwork(84, 4, env.action_space.n, global_step, scope='estimator', summaries_dir=experiment_dir)\n",
    "target_network = Qnetwork(84, 4, env.action_space.n, global_step, scope='target')\n",
    "\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=500000)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # copy network to target\n",
    "    copy_model_parameters(sess, q_network, target_network)\n",
    "    rewards, lengths = train_dqn(sess=sess,\n",
    "              env=env,\n",
    "              q_network=q_network,\n",
    "              target_network=target_network,\n",
    "              state_processor=state_processor,\n",
    "              num_episodes=1000,\n",
    "              global_step=global_step,\n",
    "              experiment_dir=experiment_dir,\n",
    "              replay_buffer=replay_buffer,\n",
    "              buffer_init_size=10000,\n",
    "              target_interval=10000,\n",
    "              frame_skip=4,\n",
    "              epsilon_start=1,\n",
    "              epsilon_end=0.1,\n",
    "              epsilon_decay_length=500000,\n",
    "              gamma=0.99,\n",
    "              batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 470.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
