{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on pacman\n",
    "\n",
    "Done with reference to Denny Britz's DQN implementation found at\n",
    "https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A replay buffer for implementing experience replay\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: the size of the replay buffer, items will be evicted in a FIFO manner\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, is_terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state\n",
    "            action\n",
    "            reward\n",
    "            next_state\n",
    "            is_terminal\n",
    "            \n",
    "        Store experience into replay buffer, evicting old experience if buffer is full\n",
    "        \"\"\"\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append([state, action, reward, next_state, is_terminal])\n",
    "        \n",
    "    def sample(self, no_of_samples):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            no_of_sample: number of samples desired\n",
    "            \n",
    "        Return:\n",
    "            samples of length no_of_sample\n",
    "            \n",
    "        Samples from replay buffer\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(len(self.buffer), no_of_samples)\n",
    "        return np.array(self.buffer)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 5, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    W = tf.get_variable('W', initializer=tf.truncated_normal(shape, mean=0, stddev=0.1))\n",
    "    return W\n",
    "\n",
    "def bias_init(shape):\n",
    "    b = tf.get_variable('b', initializer=tf.constant(0.1, shape=shape))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork:\n",
    "    def __init__(self, frame_size, no_of_frame, no_of_actions, global_step, scope='Estimator', summaries_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_size: width and height of a single frame\n",
    "            no_of_frame: number of frames stacked\n",
    "            no_of_actions: number of actions (i.e output neurons), this varies from game to game\n",
    "            scope: name of scope. Used to distinguish target and estimator network\n",
    "        \"\"\"\n",
    "        self.scope = scope\n",
    "        self.summary_writer = None\n",
    "        self.step = 0\n",
    "        with tf.variable_scope(scope):\n",
    "            # build summary writer\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "            \n",
    "            \n",
    "            self.X = tf.placeholder(tf.uint8, shape=[None, frame_size, frame_size, no_of_frame], name='X')\n",
    "            self.targets = tf.placeholder(tf.float32, shape=[None], name='targets')\n",
    "            self.selected_actions = tf.placeholder(tf.int32, shape=[None], name='actions')\n",
    "            X = tf.to_float(self.X) / 255.0\n",
    "            batch_size = tf.shape(self.X)[0]\n",
    "            \n",
    "            with tf.variable_scope('conv1'):\n",
    "                W1 = weights_init([8,8, no_of_frame, 32])\n",
    "                b1 = bias_init([32])\n",
    "                conv1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1,4,4,1], padding='VALID') + b1)\n",
    "            with tf.variable_scope('conv2'):\n",
    "                W2 = weights_init([4,4, 32, 64])\n",
    "                b2 = bias_init([64])\n",
    "                conv2 = tf.nn.relu(tf.nn.conv2d(conv1, W2, strides=[1,2,2,1], padding='VALID') + b2)\n",
    "            with tf.variable_scope('conv3'):\n",
    "                W3 = weights_init([3,3, 64, 64])\n",
    "                b3 = bias_init([64])\n",
    "                conv3 = tf.nn.relu(tf.nn.conv2d(conv2, W3, strides=[1,1,1,1], padding='VALID') + b3)\n",
    "                final_conv_width = (((frame_size - 8)//4 + 1 - 4)//2 + 1 - 3) + 1\n",
    "            with tf.variable_scope('fc4'):\n",
    "                W4 = weights_init([final_conv_width**2 * 64, 512])\n",
    "                b4 = bias_init([512])\n",
    "                flattened = tf.reshape(conv3, [-1, final_conv_width**2 * 64])\n",
    "                fc4 = tf.nn.relu(tf.matmul(flattened, W4) + b4)\n",
    "            with tf.variable_scope('fc5'):\n",
    "                W5 = weights_init([512, no_of_actions])\n",
    "                b5 = bias_init([no_of_actions])\n",
    "                self.predictions = tf.matmul(fc4, W5) + b5\n",
    "\n",
    "            # compute loss\n",
    "            # we need to extract the action values of selected actions\n",
    "            # to do that, we will flatten the predictions into a 1d array\n",
    "            # we then transform the action index to an index compatible with this 1d array\n",
    "            # we transform the action index by adding the action index to the offset for each row\n",
    "            # reference from\n",
    "            # https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "            with tf.variable_scope('loss'):\n",
    "                gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.selected_actions\n",
    "                self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "                self.losses = tf.squared_difference(self.targets, self.action_predictions)\n",
    "                self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "            # create train op\n",
    "            # I am neglecting error clipping that was used in the paper\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "                self.train_op = self.optimizer.minimize(self.loss, \n",
    "                                                        var_list=[W1, b1, W2, b2, W3, b3, W4, b4, W5, b5],\n",
    "                                                        global_step=global_step)\n",
    "            \n",
    "            # Summaries for Tensorboard\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar(\"loss\", self.loss),\n",
    "                tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "                tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "                tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "            ])\n",
    "            \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, frame_size, frame_size, no_of_frames]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, no_of_actions] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X: s })\n",
    "\n",
    "    def update(self, sess, s, a, targets):\n",
    "        \"\"\"\n",
    "        Updates the network towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, frame_size, frame_size, no_of_frames]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          targets: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X: s, self.targets: targets, self.selected_actions: a }\n",
    "        summaries, _, loss, step = sess.run(\n",
    "            [self.summaries, self.train_op, self.loss, global_step],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, step)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that Qnetwork works correctly\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\")\n",
    "q = Qnetwork(84, 4, 4, global_step, scope='test')\n",
    "sp = StateProcessor()\n",
    "env = gym.make('MsPacman-v0')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] *4, axis=2)\n",
    "    print(observation.shape)\n",
    "    observations = np.array([observation] * 32)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(q.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.ones(32)\n",
    "    a = np.array(np.random.randint(4, size=(32)))\n",
    "    print(q.update(sess, observations, a, y))\n",
    "    print(sess.run(global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb\n",
    "def copy_model_parameters(sess, network1, network2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    n1_params = [t for t in tf.trainable_variables() if t.name.startswith(network1.scope)]\n",
    "    n1_params = sorted(n1_params, key=lambda v: v.name)\n",
    "    n2_params = [t for t in tf.trainable_variables() if t.name.startswith(network2.scope)]\n",
    "    n2_params = sorted(n2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for n1_v, n2_v in zip(n1_params, n2_params):\n",
    "        op = n2_v.assign(n1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'global_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-582e93cc5d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'global_step'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    q1 = Qnetwork(84, 4, 4, scope='test1')\n",
    "    q2 = Qnetwork(84, 4, 4, scope='test2')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    copy_model_parameters(sess, q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,4], [3,4,6]]\n",
    "np.stack([a]*2, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2], [3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([a] * 4, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(sess, env, q_network, state, epsilon, e_greedy=True):\n",
    "    if e_greedy and np.random.uniform() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action_values = q_network.predict(sess, np.expand_dims(state, 0))\n",
    "        return np.argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(sess,\n",
    "              env,\n",
    "              q_network,\n",
    "              target_network,\n",
    "              state_processor,\n",
    "              num_episodes,\n",
    "              global_step,\n",
    "              experiment_dir,\n",
    "              replay_buffer,\n",
    "              buffer_init_size,\n",
    "              target_interval,\n",
    "              frame_skip,\n",
    "              epsilon_start,\n",
    "              epsilon_end,\n",
    "              epsilon_decay_length,\n",
    "              gamma,\n",
    "              batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        buffer_init_size: no of frames used to initialize replay buffer\n",
    "        frame_skip: no. of frames to skip between decision (paper used 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    episode_lengths=np.zeros(num_episodes)\n",
    "    episode_rewards=np.zeros(num_episodes)\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "    total_t = sess.run(global_step)\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_length)\n",
    "    \n",
    "    last_4_frame = []\n",
    "    \n",
    "    # init replay buffer\n",
    "    print(\"Populating replay memory...\")\n",
    "    observation = env.reset()\n",
    "    observation = state_processor.process(sess, observation)\n",
    "    # populate last 4 frames\n",
    "    last_4_frame = [observation.tolist()] * 4\n",
    "    \n",
    "    for i in range(buffer_init_size):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"\\rpopulating buffer: %d\" %i, end=\"\")\n",
    "        current_state = np.stack(last_4_frame, axis=2)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = state_processor.process(sess, observation)\n",
    "        last_4_frame.pop(0)\n",
    "        last_4_frame.append(observation)\n",
    "        next_state = np.stack(last_4_frame, axis=2)\n",
    "        replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "            observation = state_processor.process(sess, observation)\n",
    "            # populate last 4 frames\n",
    "            last_4_frame = [observation.tolist()] * 4\n",
    "            \n",
    "        \n",
    "    print(\"\\nPopulated replay memory\")\n",
    "    \n",
    "    # this can only be done after populating replay buffer\n",
    "    should_record = False\n",
    "    # set up env to record video near the end of training\n",
    "    env = Monitor(env, directory='./', resume=True, video_callable=lambda count: should_record and count % 10 == 0)\n",
    "    \n",
    "    \n",
    "    action = None\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        # if is last 4 episode, record video\n",
    "        if i_episode >= num_episodes - 4 or i_episode % 1000 == 0:\n",
    "            should_record = True\n",
    "        else:\n",
    "            should_record = False\n",
    "\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        observation = state_processor.process(sess, observation)\n",
    "        last_4_frame = [observation.tolist()] * 4\n",
    "        loss = None\n",
    "        current_state = np.stack(last_4_frame, axis=2)\n",
    "        for t in itertools.count():\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_length-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_network.summary_writer.add_summary(episode_summary, total_t)\n",
    "            \n",
    "            # update the target network\n",
    "            if total_t % target_interval == 0:\n",
    "                copy_model_parameters(sess, q_network, target_network)\n",
    "                \n",
    "            \n",
    "            if t % frame_skip == 0:\n",
    "                # only make decision every k (frame_skip) steps\n",
    "                action = select_action(sess, env, q_network, current_state, epsilon, e_greedy=True)\n",
    "                \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            observation = state_processor.process(sess, observation)\n",
    "            last_4_frame.pop(0)\n",
    "            last_4_frame.append(observation)\n",
    "            next_state = np.stack(last_4_frame, axis=2)\n",
    "            replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update statistics\n",
    "            episode_rewards[i_episode] += reward\n",
    "            episode_lengths[i_episode] = t\n",
    "            \n",
    "            # train\n",
    "            samples = replay_buffer.sample(batch_size)\n",
    "            states = []\n",
    "            actions = []\n",
    "            targets = []\n",
    "            for s, a, r, next_s, d in samples:\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                # compute target\n",
    "                qs = target_network.predict(sess, np.expand_dims(next_s, 0))\n",
    "                qmax = np.max(qs)\n",
    "                if r > 0:\n",
    "                    r_clip = 1\n",
    "                elif r < 0:\n",
    "                    r_clip = -1\n",
    "                else:\n",
    "                    r_clip = 0\n",
    "                targets.append(r_clip + gamma * qmax * (1-int(d)))\n",
    "            loss = q_network.update(sess, states, actions, targets)\n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            current_state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        print(\"\\nEpisode {}, Reward: {}\".format(i_episode + 1, episode_rewards[i_episode]))\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_network.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_network.summary_writer.flush()\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-04 18:19:05,810] Making new env: MsPacman-v0\n",
      "/home/kai/rl/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /home/kai/rl/gym/dqn/experiments/MsPacman-v0/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/kai/rl/gym/dqn/experiments/MsPacman-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-04 18:19:07,306] Restoring parameters from /home/kai/rl/gym/dqn/experiments/MsPacman-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "populating buffer: 9000\n",
      "Populated replay memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-04 18:19:36,069] Starting new video recorder writing to /home/kai/rl/gym/dqn/openaigym.video.0.2601.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 581 (555025) @ Episode 1/1000, loss: 0.39982494711875916\n",
      "Episode 1, Reward: 340.0\n",
      "Step 579 (555605) @ Episode 2/1000, loss: 0.18421803414821625\n",
      "Episode 2, Reward: 370.0\n",
      "Step 511 (556117) @ Episode 3/1000, loss: 0.20109382271766663\n",
      "Episode 3, Reward: 170.0\n",
      "Step 932 (557050) @ Episode 4/1000, loss: 0.34056532382965095"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-41d662723007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mepsilon_decay_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               batch_size=32)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a52596a8458e>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(sess, env, q_network, target_network, state_processor, num_episodes, global_step, experiment_dir, replay_buffer, buffer_init_size, target_interval, frame_skip, epsilon_start, epsilon_end, epsilon_decay_length, gamma, batch_size)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;31m# compute target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mqmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e35983556c95>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sess, s)\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0maction\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1105\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mdirect\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \"\"\"\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3268\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3269\u001b[0m     \"\"\"\n\u001b[0;32m-> 3270\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3272\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Unfortunately, this still doesn't provide good help output when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# inspecting the created context manager instances, since pydoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "env = gym.make('MsPacman-v0')\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "q_network = Qnetwork(84, 4, env.action_space.n, global_step, scope='estimator', summaries_dir=experiment_dir)\n",
    "target_network = Qnetwork(84, 4, env.action_space.n, global_step, scope='target')\n",
    "\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=500000)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # copy network to target\n",
    "    copy_model_parameters(sess, q_network, target_network)\n",
    "    rewards, lengths = train_dqn(sess=sess,\n",
    "              env=env,\n",
    "              q_network=q_network,\n",
    "              target_network=target_network,\n",
    "              state_processor=state_processor,\n",
    "              num_episodes=1000,\n",
    "              global_step=global_step,\n",
    "              experiment_dir=experiment_dir,\n",
    "              replay_buffer=replay_buffer,\n",
    "              buffer_init_size=10000,\n",
    "              target_interval=10000,\n",
    "              frame_skip=1,\n",
    "              epsilon_start=1,\n",
    "              epsilon_end=0.1,\n",
    "              epsilon_decay_length=500000,\n",
    "              gamma=0.99,\n",
    "              batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 470.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
